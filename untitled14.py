# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pfFGyOg6vY0a1OrnHOo2RYLKMPfIyitz
"""

!pip install pymongo

import pandas as pd
from pymongo import MongoClient
import time
import json
from bson import ObjectId
from datetime import datetime
import os
import time
import json
import sys

client = MongoClient("mongodb+srv://ayushlokhande0:NufT140wTqeUcfFa@cluster0.803ckl6.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0")

db = client['amenda_010']

collection = db['alex']

documents = collection.find()

docs_list = list(documents)

df = pd.DataFrame(docs_list)

print(df.head())



start_time = time.time()

document = collection.find()
docs_list = list(document)

end_time = time.time()
query_time = end_time - start_time

df = pd.DataFrame(docs_list)

print(df.head())



total_size = sum(len(json.dumps(doc)) for doc in docs_list)

total_size = sum(len(json.dumps(doc)) for doc in docs_list)

def serilaize_doc(doc):
  if '_id' in doc:
    doc['_id'] = str(doc['_id'])
  return doc

import json
from datetime import datetime

def serilaize_doc(doc):
  if '_id' in doc:
    doc['_id'] = str(doc['_id'])
  # Handle datetime objects
  for key, value in doc.items():
      if isinstance(value, datetime):
          doc[key] = value.isoformat()  # Convert datetime to ISO format string
  return doc

total_size = sum(len(json.dumps(serilaize_doc(doc))) for doc in docs_list)
print(total_size)

query_latency = end_time - start_time

throughput = total_size / query_latency if query_latency > 0 else float('inf')

metrics_report = {
    "total_documents": len(docs_list),
    "data_size": f"{total_size} bytes",
    "query_latency": f"{query_latency:.6f} seconds",
    "throughput": f"{throughput:.2f} bytes/second"
}

print("\nData Transfer Metrics Report:")
for metric, value in metrics_report.items():
    print(f"{metric}: {value}")



start_time = time.time()

document = collection.find()
docs_list = list(document)

metrics_df = pd.DataFrame([metrics_report])

CSV_FILE_PATH = "metrics_report.csv"

if os.path.exists(CSV_FILE_PATH):
  existing_metrics_df = pd.read_csv(CSV_FILE_PATH)

if os.path.exists(CSV_FILE_PATH):
  metrics_df.to_csv(CSV_FILE_PATH, mode='a', header=False, index=False)
else:
  metrics_df.to_csv(CSV_FILE_PATH, index=False)



print("\nMerics saved to performance_metrics.csv")

CSV_FILE_PATH = "metrics_report.csv"

metrics_df = pd.read_csv(CSV_FILE_PATH)

print(metrics_df.head(10))

current_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

metrics_report = {
    "timestamp": current_timestamp,
    "total_documents": len(docs_list),
    "data_size": total_size,
    "query_latency": query_latency,
    "throughput": throughput
}

metrics_df = pd.DataFrame([metrics_report])

csv_file_path = "performance_metrics.csv"

if os.path.exists(csv_file_path):
    # Append to the existing CSV file
    metrics_df.to_csv(csv_file_path, mode='a', header=False, index=False)
else:
    # Create a new CSV file with headers
    metrics_df.to_csv(csv_file_path, mode='w', header=True, index=False)

print("\nMetrics saved to performance_metrics.csv")

CSV_FILE_PATH = "performance_metrics.csv"

metrics_df = pd.read_csv(CSV_FILE_PATH)

print(metrics_df.head(10))

from google.colab import files

files.download('/content/performance_metrics.csv')

